# This code uses data from stathead to predict post season winning percentage with regular season data from the same year
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn import preprocessing
import tensorflow as tf

def random_forest(train_features, test_features, train_labels, test_labels):
  rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)
  rf.fit(train_features, np.ravel(train_labels));

  predictions = rf.predict(test_features)
  errors = abs(predictions - test_labels)
  print('Mean Absolute Error:', round(np.mean(errors), 2))

  importances = list(rf.feature_importances_)
  # List of tuples with variable and importance
  feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(headers, importances)]
  # Sort the feature importances by most important first
  feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)
  # Print out the feature and importances
  [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];

def neural_net(train_features, test_features, train_labels, test_labels):
  model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(units=64, activation='relu',
                          input_shape=train_features.shape),
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dense(units=1)
  ])

  model.compile(optimizer='adam',
              loss='mae',
              metrics=['accuracy'])

  model.fit(train_features, train_labels, epochs=5)

  model.evaluate(test_features,  test_labels, verbose=2)

data = pd.read_csv('/content/drive/MyDrive/NBA team season data.csv')

features_all = data.iloc[:,3:27]
labels_all = data.iloc[:,30:54]
headers = ["TS", "TO", "OREB", "DREB", "FT"]

labels_win_percent = labels_all.iloc[:,4:5]
labels_wins = labels_all.iloc[:,0:1]
labels_net_rating = labels_all.iloc[:,11:12]
features_win_percent = features_all.iloc[:,4:5]
features_point_diff = features_all.iloc[:,5:6]
features_srs = features_all.iloc[:,7:8]
features_net_rating = features_all.iloc[:,11:12]
features_true_shooting = features_all.iloc[:,13:14]
features_turnovers = features_all.iloc[:,14:15]
features_off_reb = features_all.iloc[:,15:16]
features_def_reb = features_all.iloc[:,16:17]
features_free_throw = features_all.iloc[:,17:18]

labels = np.array(labels_win_percent)
features_concat = pd.concat([features_true_shooting, features_turnovers, features_off_reb, features_def_reb, features_free_throw], axis=1)
features_original = np.array(features_win_percent)

features = preprocessing.normalize(features_original)

#print("features shape: ", features.shape)
#print("labels shape: ", labels.shape)

train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)

#print("train_features shape: ", train_features.shape)
#print("train_labels shape: ", train_labels.shape)
#print("test_features shape: ", test_features.shape)
#print("test_labels shape: ", test_labels.shape)

baseline_preds = np.full((labels.size,1), np.average(labels))
baseline_error = abs(labels - baseline_preds)

print('Average baseline error: ', round(np.mean(baseline_error), 2))

neural_net(train_features, test_features, train_labels, test_labels)
random_forest(train_features, test_features, train_labels, test_labels)
